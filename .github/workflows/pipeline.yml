name: Tourism Project Pipeline

on:
  push:
    branches:
      - main

jobs:

  register-dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: pip install pandas datasets huggingface_hub
      - name: Upload Dataset to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python src/register_dataset.py

  data-prep:
    needs: register-dataset
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: pip install pandas datasets scikit-learn huggingface_hub
      - name: Run Data Preparation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python src/data_prep.py
      # We upload the clean data as an artifact so the next job can see it
      - name: Upload Clean Data Artifact
        uses: actions/upload-artifact@v4
        with:
          name: clean-data
          path: data/tourism_cleaned.csv

  model-training:
    needs: data-prep
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Download Clean Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: clean-data
          path: data
      - name: Install Dependencies
        run: pip install pandas scikit-learn joblib huggingface_hub matplotlib seaborn
      - name: Start MLflow Server (Simulation)
        run: |
          pip install mlflow
          nohup mlflow ui --host 0.0.0.0 --port 5000 &
          sleep 5
      - name: Model Building
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python src/model_training.py

  deploy-hosting:
    runs-on: ubuntu-latest
    needs: [model-training]
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: pip install huggingface_hub
      - name: Push files to Frontend Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python src/deploy.py
